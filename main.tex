%%%%%%%%% Beginning of the Preamble %%%%%%%%%
\documentclass[11pt,twoside,a4paper,english]{article}

\def\eg{e.g., }
\def\ie{i.e., }
\def\og{``}
\def\fg{"}

\newcommand{\myparagraph}[1]{{\vspace{0cm}\textbf{#1}\vspace{-0.3cm}}}
\usepackage{graphicx}
\graphicspath{{figures/}}

\usepackage[affil-it]{authblk}
\usepackage[space]{grffile}
\usepackage{latexsym}
\usepackage{textcomp}
\usepackage{longtable}
\usepackage{multirow,booktabs}
\usepackage{amsfonts,amsmath,amssymb}
\usepackage{url}
\usepackage{hyperref}
%\usepackage{subcaption}
\hypersetup{colorlinks=false,pdfborder={0 0 0}}
%\usepackage{latexml}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{lipsum}
\usepackage{fancyhdr}
% Added packages Moret
%\usepackage{fixltx2e} % not necessary for the latest releases
\usepackage[usenames,dvipsnames]{color} % for color text
\usepackage{comment} % To comment out blocks of text
\usepackage{float} % To put figures/tables exactly where I want them
\usepackage{tablefootnote} % To add footnotes below tables
\usepackage{scrextend} % to use \footref: multiple reference to the same table footnote
\usepackage{pbox} % to have new line inside table cells
\usepackage{fullpage} % To use extended margins 
\usepackage{gensymb} % to have the ¡ symbol
\usepackage{epstopdf}
\usepackage{subfigure}
\usepackage{multirow}
\usepackage{mathtools} % To use mathclap in equations
\usepackage{bm} % To use \bm in order to get bold math symbols


% Bibliography: only initials
\usepackage[natbib = true,backend=bibtex,  sorting=none,giveninits=true,style=numeric-comp,maxcitenames=1,maxbibnames=6]{biblatex}
\bibliography{supplementary/Biblio}


\usepackage{titlesec} % to associate numbers to paragraphs (mimicking subsubsubsections)
\setcounter{secnumdepth}{4} % to associate numbers to paragraphs (mimicking subsubsubsections)
%%%%%%%%% End of the Preamble %%%%%%%%%

	
%%%%%%%%% GL Packages   %%%%%%%%%%%
\usepackage[acronym,nonumberlist]{glossaries} 
\usepackage{glossary-mcols}  
\usepackage{glossary-longragged}
%\usepackage{amssymb}
\usepackage{lineno}
\usepackage{longtable}
\usepackage[font=small,skip=2pt]{caption}
%\usepackage{amsmath}
%\usepackage{amssymb}
%\usepackage{eurosym}
%\usepackage{graphics}
%\usepackage{multirow}
%\usepackage{url}
%\usepackage{booktabs}
\usepackage[version=4]{mhchem}
\usepackage{siunitx}
%\usepackage{glossaries}
%\usepackage{varioref}
%\usepackage{hyperref}
\usepackage{cleveref}
%\usepackage{subfigure}
%
%\usepackage{lscape}
%\usepackage{rotating}
%\usepackage{pdflscape} 
\usepackage{pifont}% http://ctan.org/pkg/pifont
\usepackage{pdfrender}
\newcommand*{\boldcheckmark}{%
  \textpdfrender{
    TextRenderingMode=FillStroke,
    LineWidth=.8pt, % half of the line width is outside the normal glyph
  }{\checkmark}%
}
\usepackage[titletoc]{appendix}% http://ctan.org/pkg/appendices
\newcommand{\xmark}{\ding{55}}%

%SM
\usepackage{pdflscape} % to use landscape environment

% Definition of Symbols
\newglossary[slg]{symbolslist}{syi}{syg}{List of Symbols}

%\newglossaryentry{e}{name=\emph{e},description={Error factor}, user1={}, type=symbolslist, sort=error}
%\newglossaryentry{theta}{name=$\theta$,description={Parameter}, user1={}, type=symbolslist, sort=hz}

%% A
\newacronym{AEO}{AEO}{Annual Energy Outlook}

%% B
\newacronym{BAU}{BAU}{Business-As-Usual}
\newacronym{BEV}{BEV}{Battery Electric Vehicles}

%% C
\newacronym{CAPEX}{CAPEX}{Capital Expenditure}
\newacronym{CCGT}{CCGT}{Combined Cycle Gas Turbine}
\newacronym{CCS}{CCS}{Carbon Capture and Storage}
\newacronym{CEPCI}{CEPCI}{Chemical Engineering's Plant Cost Index}
\newacronym{CHP}{CHP}{Combined Heat and Power}
\newacronym{CNG}{CNG}{Compressed Natural Gas}
\newacronym{CO2}{CO\textsubscript{2}}{Carbon Dioxyde}
\newacronym{COP}{COP}{Coefficient of Performance}

%% D
\newacronym{DEC}{DEC}{Decentralized}
\newacronym{DHN}{DHN}{District Heating Network}
\newacronym{DM}{DM}{Decision-Maker}
\newacronym{DNN}{DNN}{Deep Neural Network}
\newacronym{DRL}{DRL}{Deep Reinforcement Learning}


%% E
\newacronym{EE}{EE}{Elementary Effect}
\newacronym{EIA}{EIA}{Energy Information Administration}
\newacronym{ES}{EnergyScope}{EnergyScope}
\newacronym{ESTD}{EnergyScope TD}{EnergyScope Typical Days}
\newacronym{EO}{EO}{Expert Opinion}
\newacronym{ESOM}{ESOM}{Energy System Optimisation Models}
\newacronym{EU}{EU}{European Union}
\newacronym{EUD}{EUD}{End-Use Demand}
\newacronym{EUT}{EUT}{End-Use Type}
\newacronym{EV}{EV}{Electric Vehicle}

%% F
\newacronym{FC}{FC}{Fuel Cell}
\newacronym{FEC}{FEC}{Final Energy Consumption}

%% G
\newacronym{GHG}{GHG}{Greenhouse Gas}
\newacronym{GSA}{GSA}{Global Sensitivity Analysis}
\newacronym{GtP}{GtP}{Gas-to-Power}
\newacronym{GWP}{GWP}{Global Warming Potential}

%% H
\newacronym{H2}{H2}{Hydrogen}
\newacronym{HEV}{HEV}{Hybrid Electric Vehicle}
\newacronym{HH}{HH}{households}
\newacronym{HP}{HP}{Heat Pump}
\newacronym{HT}{HT}{High-Temperature}
\newacronym{HVC}{HVC}{High Value Chemicals}
\newacronym{HW}{HW}{Hot Water}

%% I
\newacronym{ICE}{ICE}{Internal Combustion Engine}
\newacronym{IEA}{IEA}{International Energy Agency}
\newacronym{IGCC}{IGCC}{Integrated Gasification Combined Cycle}
\newacronym{IPCC}{IPCC}{Intergovernmental Panel for Climate Change}

%% J

%% K
\newacronym{KPI}{KPI}{Key Performance Indicator}

%% L
\newacronym{LCA}{LCA}{Life Cycle Assessment}
\newacronym{LCOE}{LCOE}{Levelised Cost of Energy}
\newacronym{LFO}{LFO}{Light Fuel Oil}
\newacronym{LHV}{LHV}{Lower Heating Value}
\newacronym{LNG}{LNG}{Liquified Natural Gas}
\newacronym{LOO}{LOO}{Leave-One-Out}
\newacronym{LP}{LP}{Linear Programming}
\newacronym{LT}{LT}{Low-Temperature}

%% M
\newacronym{MDP}{MDP}{Markov Decision Process}
\newacronym{MILP}{MILP}{Mixed-Integer Linear Programming}
\newacronym{MOB}{MOB}{Mobility}
\newacronym{MPG}{MPG}{miles-per-gallon}
\newacronym{MSW}{MSW}{Municipal Solid Waste}
\newacronym{MTO}{MTO}{Methanol-to-Olefins}

%% N
\newacronym{NED}{NED}{Non-Energy Demand}
\newacronym{NG}{NG}{Natural Gas}
\newacronym{NN}{NN}{Neural Network}

%% O
\newacronym{OM}{O\&M}{Operation and Maintenance}
\newacronym{Openmod}{Openmod}{Open Energy Modelling Initiative}
\newacronym{OPEX}{OPEX}{Operational Expenditure}
\newacronym{ORC}{ORC}{Organic Rankine cycle}

%% P
\newacronym{PCE}{PCE}{Polynomial Chaos Expansion}
\newacronym{PDF}{PDF}{Probability Density Function}
\newacronym{PF}{PF}{Perfect Foresight}
\newacronym{PHEV}{PHEV}{Plug-in Hybrid Electric Vehicle}
\newacronym{PHS}{PHS}{Pumped Hydro Storage}
\newacronym{pkm}{pkm}{passenger-kilometer}
\newacronym{PtG}{PtG}{Power-to-Gas}
\newacronym{PtH}{PtH}{Power-to-Heat}
\newacronym{PV}{PV}{Photovoltaic}

%% Q

%% R
\newacronym{RL}{RL}{Reinforcement Learning}

%% S
\newacronym{SFOE}{SFOE}{Swiss Federal Office of Energy}
\newacronym{SFOS}{SFOS}{Swiss Federal Office of Statistics}
\newacronym{SH}{SH}{Space Heating}
\newacronym{SMR}{SMR}{Small Modular Reactor}
\newacronym{SNG}{SNG}{Synthetic Natural Gas}
\newacronym{SAC}{SAC}{Soft Actor-Critic}
\newacronym{SPC}{SPC}{Sparse Polynomial Chaos}

%% T
\newacronym{TCO}{TCO}{Total Cost of Ownership}
\newacronym{TD}{TD}{Typical Day}
\newacronym{tkm}{tkm}{ton-kilometer}
\newacronym{TS}{TS}{Thermal Storage}
\newacronym{TSO}{TSO}{Transmission System Operator}

%% U
\newacronym{U-S}{U-S}{Ultra-Supercritical}
\newacronym{UQ}{UQ}{Uncertainty Quantification}

%% V
\newacronym{V2G}{V2G}{Vehicle-to-Grid}
\newacronym{VRES}{VRES}{Variable Renewable Energy Sources}

%% W

%% X

%% Y

%% Z


%\usepackage{subcaption}

\usepackage{tablefootnote}

\usepackage{makecell}


\makeglossaries

%%%%%%%%% End of the Glossary Stuff %%%%%%%%%

%%%%%%%%% Beginning of the Report %%%%%%%%%
\begin{document}

%%%%%%%%% Beginning of the Title Page %%%%%%%%%
\begin{titlepage}

% To add the logos
%\begin{figure*}[!htb]
%\centering
%\subfigure{\includegraphics[width=4cm]{figures/logos/logo_epfl.eps}}\hfill
%\quad
%\subfigure{\includegraphics[width=4.6cm]{figures/logos/logo_ipese.eps}}
%\end{figure*}

% To add the title
\title{Agent-based reinforcement learning to support myopic energy transition}

% To add the authors
\author[1]{Xavier Rixhon\thanks{xavier.rixhon@uclouvain.be}}
\author[1]{Hervé Jeanmart}
\author[1]{Francesco Contino}


%To add the affiliations
\affil[1]{Institute of Mechanics, Materials and Civil Engineering, 
Université catholique de Louvain (UCLouvain), Place du Levant, 2, 1348 Louvain-la-Neuve, Belgium}





\date{} %add date if you want to display it in the cover page
{\let\newpage\relax\maketitle}

% To add content to the title page
%\setcounter{tocdepth}{2}
\tableofcontents
\printglossaries
% To add footnote to the title page

\end{titlepage}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%           CORE TEXT           %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\linenumbers
%\linenumbers

% This is a summary to communicate to non-experts the context and implications of the research, potential policy implications and recommendations, and the challenges and opportunities in scaling up or down the conclusions presented. The context & scale statement may be somewhat speculative in nature. It should, however, be substantiated by the key results presented in the article. The context & scale statement should be 1 or 2 paragraphs with a maximum of 1000 characters including spaces.
\section{Context and scale}

% Abstract
\section{Summary}

\section{Keywords}
Whole-energy systems, transition pathways, energy policy, reinforcement learning, optimisation, uncertainty, myopic decision-making.
% Background, Methods, Results, and Conclusions

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%           INTRO           %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Introduction}
\label{sec:intro}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%           RESULTS  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

Due to the increasing complexity of the systems and the integration of uncertainties, the last decades have seen the emergence of publications where \gls{RL} is applied to energy systems \cite{cao2020reinforcement,perera2021applications}. In their respective reviews, \citet{cao2020reinforcement} and \citet{perera2021applications} highlighted groups of problems addressed with \gls{RL} in the research field of energy systems: building energy management system (BEMS), optimisation of dispatch and operational control closely linked with the energy market and the optimal power flow problem in the grid, micro-grid management, electro-mobility or even demand-side management or optimal control of energy system devices like maximum power point tracking (MPPT) of wind turbines and \gls{PV} panels.  

In this paper, we propose a novel method based on the application of \gls{RL} to a new kind of energy system problem: the optimisation of the transition pathway of a whole-energy system . In this sense, the objective is to optimise a policy to support this transition in myopic conditions and subject to uncertainties and a \ce{CO2} budget rather than a prescribed \gls{GHG} emissions trajectory (see Figure \ref{fig:Schematics_RL}). This method highlights the importance of taking short-term actions and points out the no-go zones where succeeding the energy transition is very unlikely.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Schematics_RL.pdf}
\caption{The \Acrfull{RL} framework applied to the myopic optimisation of the energy transition pathway between 2020 and 2050. Here, the agent interacts with its environment, \ie the energy-system model on a limited decision window of 10 years. At the beginning of each episode, a different sample of uncertain parameters is drawn and affects the environment, EnergyScope Pathway, according to the methodology detailed in Section \ref{subsec:uncert_charac}.}
\label{fig:Schematics_RL}
\end{figure}

\section{Results}
\label{sec:results}

\subsection{Reward and success}
\label{subsec:RL:learning:rew_succ}

The learning phase has been split into batches of 500 steps, \ie 500 sequences of state-action-reward-new state. %For the first 100 steps of each batch, the \gls{RL}-model collects transitions before learning starts. This makes sure replay buffer is full enough for useful updates. 
At the end of each batch, the up-to-date policy, \ie the \gls{NN}, is saved. This way, we can assess the progress in the learning process and its convergence (see Figure \ref{fig:reward_success}).  The mean reward increases rapidly at the beginning of the learning process before reaching a plateau where the optimisation of the policy becomes more marginal.  As these successes indirectly drive the agent's optimisation, it shows that the reward function (see Figure \ref{fig:Reward}) leads towards more and more successes. However, given the wide range of uncertainty of some parameters and the agent's levers of action, this success rate stays limited at the end of the learning process. In other words, there are conditions where it is impossible for the agent to succeed the transition.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.49\textwidth]{Mean_reward.pdf}
\includegraphics[width=0.49\textwidth]{Success_rate.pdf}
\caption{Mean reward and success rate of the different learning batches. The stabilisation of the reward curve shows a convergence of the learning process from the agent's point of view. The evolution of the success rate also shows that the reward function aims at more and more successful transitions.}
\label{fig:reward_success}
\end{figure}

\newpage
When assessing the distributions of the values of reward in the failure and success cases, one notices a range where these distributions overlap (see left-hand side of Figure \ref{fig:reward_status}). This area corresponds to either transitions that exceed the \ce{CO2} budget in 2050 but are cheaper than the total transition cost of reference (see Section \ref{sec:RL:act_states_rew}) or successful transitions that are more expensive. Besides this overlap, we observe that successes account for the majority of the cases with higher rewards. This is another indication that the reward function is appropriate in this exploration of successful transition pathways. More indirectly, in case of applying this methodology to another case study, this substantiates that the weights between emissions and cost defined in Section \ref{subsec:RL:act_states_rew:rew} could be used as an initial step and fine-tuned afterwards.

Considering the end of the time window where the \ce{CO2} budget is exceeded, the right-hand side of Figure \ref{fig:reward_status} shows that 2040 is the ``tipping year'' for the agent. Beyond this point, through this learning process, the chances to succeed the transition were 38\%. In other words, near-term (2025-2030) actions are necessary to hope to succeed the transition.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.49\textwidth]{Reward_status.pdf}
\includegraphics[width=0.49\textwidth]{Reward_status_2.pdf}
\caption{Reward distribution between successes and failures. Graph on the right-hand side details at the end of which time window the failure occurred. The ``tipping year'' is 2040 as failing the transition by 2040 represents 57\% of all the failures. Beyond this point, through this learning process, succeeding the transition represents 38\% of the episodes. }
\label{fig:reward_status}
\end{figure}

\subsection{States}
\label{subsec:RL:learning:states}

The first two dimensions of the state space are the cumulative emissions and costs. They drive the value of the reward and, consequently, the optimisation of the agent's policy. Per definition, the threshold of 1.2\,Gt$_{\ce{CO2},\text{eq}}$ splits the episodes reaching 2050 into successes and failures (see Figure \ref{fig:Cum_gwp_cost}). Since infeasible cases or those that overshoot the \ce{CO2} budget are discarded before 2050 (see Section \ref{subsec:RL:act_states_rew:rew}), the number of attempts that reach further steps in the transition progressively decreases. Consequently, the share of successful transitions compared to failures progressively increases with time. 

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Cum_gwp_cost_3.pdf}
\caption{Exploration of the state space over the learning process: distribution of occurrence of cumulative emissions (left) and costs (right). The number of remaining attempts decreases with time since the infeasible problems and the solutions overshooting the \ce{CO2} budget are discarded prematurely, \ie before 2050. Besides infeasible problems, distributions labelled as ``Failure'' represent the attempts that overshot the \ce{CO2} budget by 2050 at the latest. The majority of successful transitions have cumulative emissions much lower than the \ce{CO2} budget and are cheaper than the REF case. }
\label{fig:Cum_gwp_cost}
\end{figure}

In the successful transitions, the median cumulative emissions, $\text{P}_{50}$, are about 0.9\,Gt$_{\ce{CO2},\text{eq}}$. Reaching cumulative emissions significantly lower than the \ce{CO2} budget is possible thanks to efforts made at earlier stages of the transition and the potential to install \gls{SMR} later on. Considering the failures in 2050, half of these episodes ended up with cumulative emissions lower or equal to 1.4\,Gt$_{\ce{CO2},\text{eq}}$. As illustrated in Figure \ref{fig:reward_status}, 2040 is identified as the tipping year. Where 98\% of the failures were below the \ce{CO2} budget in 2035, only 37\% passed this threshold in 2040. This reminds the importance of near-term, 2025-2030, actions to hope for a successful transition.

Given the reward function (see Figure \ref{fig:Reward}), the agent optimises its policy by aiming at lowering the total transition cost as soon as it meets the \ce{CO2} budget. The skewness of the cumulative emissions and costs in 2050 are indications of this reward function (see Table \ref{tab:skewness_gwp_cost}). When succeeding the transitions, the cumulative emissions have a negative skewness: the agent successfully stayed within the budget and most of the cases were close to that budget (median at 0.9 Gt$_{\ce{CO2},\text{eq}}$). On the contrary, the cumulative cost of successful transitions has a positive skewness: the agent successfully reduces the cost of the system as a secondary objective with 30\% of the cases above the reference transition cost. The hierarchy of the agent's objectives is verified with the failures. When it failed the transition, the agent aimed at reducing the emissions (skewness of 0.61) before minimising the total transition cost (skewness of 0.24). 

\begin{table}[htbp!]
\caption{Skewness of cumulative emissions and costs in 2050. Cumulative emissions are skewed to the left and to the right for the successes and failures, respectively. The skewness of the cumulative costs for successful transitions is higher compared to failures. On top of being the results of the optimisation through EnergyScope, these are influenced by the agent's policy that aims only at lowering the total transition cost as soon as it meets the \ce{CO2} budget.}
\label{tab:skewness_gwp_cost}
\centering
\begin{tabular}{l c c}
\toprule
\textbf{Status of episode}  & \textbf{Skewness of cumulative} & \textbf{Skewness of cumulative} \\
\textbf{in 2050}  & \textbf{emissions} & \textbf{costs} \\	
\midrule
Success & -0.52 & 0.50 \\
Failure & 0.61 & 0.24 \\
\bottomrule							

\end{tabular}
\end{table}

Finally, we observe that the majority of the successful transitions are cheaper than the reference transition cost, 1.1\,b€. Among the parameters impacting the most the total transition cost, we observe that success occurs when, on average,  the cost of purchasing fossil fuels is increased more than the one of electrofuels (see Table \ref{tab:param_RL}). In other words, to have higher chances to succeed a myopic transition, the key factor is to reduce the uncertainty on the cost of purchasing electrofuels or to increase the cost of fossil fuels. Given the skewness that is positive and negative for the electrofuels and the fossil fuels, respectively, these cases represent more than the majority of the successful cases. On top of this, total transition costs of successful episodes are lower due to lower industrial \gls{EUD} and discount rates. These favourable conditions combined with the right agent's actions led to transitions respecting the \ce{CO2} budget.

\begin{table}[htbp!]
\caption{Uncertain parameters impacting the most the total transition cost and, for the successful transitions, the mean of their values between 0 and 100\%, $\mu$, and their skewness, $\gamma$. On top of being supported by the agent's actions, successful transitions occur when the cost of purchasing fossil fuels is more increased than the one of electrofuels.}
\label{tab:param_RL}
\centering
\begin{tabular}{l c c c}
\toprule
\textbf{Parameter}  & \textbf{$\mu$} & \textbf{$\gamma$}  \\	
\midrule
Purchase electrofuels & 50.4\% & 0.004  \\
Industry EUD & 49.8\% & 0.026 \\
Discount rate & 48.4\% & 0.089\\
Purchase fossil fuels  & 55.0\% & -0.068\\
\bottomrule							

\end{tabular}
\end{table}

Besides the cumulative emissions and costs, the agent also observes the share of renewable energy carriers in the primary mix and the efficiency of the system. The share of renewable energy carriers in the primary mix allows identifying intermediate milestones along successful transitions (see Figure \ref{fig:RE_in_mix_Efficiency}). From the initial state of 10\% in 2020, a boost of integration of renewables in the near term is needed to hope for a successful transition. For the successful occurrences to exceed failures, this share increases to 54\% in 2025. Along the transitions, this increase goes with the import of electrofuels and the full deployment of local \gls{VRES}. In 2050, the threshold where successes occur more often than failures was at 82\% renewable share. In the REF case of Chapter \ref{chap:atom_mol}, this share reached 86\% by 2050. However, by 2050, Figure \ref{fig:RE_in_mix_Efficiency} shows another ``bump'' at lower shares of renewables in the mix. This area corresponds to the possibility of installing \gls{SMR}. As uranium is considered as a non-renewable resource \cite{rixhon2021terminology}, installing \gls{SMR} allows lowering the threshold as in the SMR case of Chapter \ref{chap:atom_mol}. Besides these milestones to respect the \ce{CO2} budget of the transition, one can also look at the other side of the thresholds. Below the near-term threshold of $\sim$60\%, this is the ``no-go zone'' where succeeding the transition becomes unlikely, except if betting on the future installation of \gls{SMR}.

The efficiency, as defined in Section \ref{sec:RL:act_states_rew}, gives less valuable information towards successful transitions. Through the transition, besides the share of success increasing over the failures, the distributions of success and failure indistinguishably spread over the whole range. Similarly to the emissions, we observe a bump at lower efficiencies by 2050 due to the installation of \gls{SMR}.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{RE_in_mix_Efficiency.pdf}
\caption{Exploration of the state space over the learning process: distribution of occurrence of share of renewable energy carriers in the primary energy mix (left) and efficiency (right).  The number of remaining attempts decreases with time since infeasible problems and solutions overshooting the \ce{CO2} budget are discarded prematurely, \ie before 2050. Besides infeasible problems, distributions labelled as ``Failure'' represent the attempts that overshot the \ce{CO2} budget by 2050 at the latest. Integration of local \gls{VRES} at early stages then massive import of electrofuels later are needed to secure successful transitions. Below a near-term threshold ($\sim$60\%), the chances of success are limited, \ie no-go zones. Efficiency is less valuable information for the agent to succeed transitions as failures and successes indistinguishably spread over the whole range.}
\label{fig:RE_in_mix_Efficiency}
\end{figure}

\subsection{Actions}
\label{subsec:RL:learning:actions}

After investigating the intermediate milestones to meet the \ce{CO2} budget by 2050, this section details the actions the agent has taken during the learning process (see Figure \ref{fig:Actions_learning}). Rows represent the beginning of the time window at which the set of actions is taken. Similarly to the state space, we observe a wide exploration of the action space. The more the agent was able to progress through transition, without exceeding the \ce{CO2} budget, the bigger is the share of successes compared to failures. Besides this observation, no specific range of values for the different actions at the different timings seems to lead to more successes. Looking at action individually, there does not seem to be any that supports more effectively the transition. The success comes from the combination of these actions. 

After filtering out failures of the learning episodes and keeping only the successful transitions, only a limited set of the actions are binding and have an actual impact on the result of the optimisation in EnergyScope Pathway (see Figure \ref{fig:Binding_learning}). Supplementary Note \ref{subsec:binding} provides further details on the binding characteristic of a constraint. This allows identifying key actions to support the myopic transition. Limiting the \gls{GWP} in the near term is a key factor for success. However, this action has a binding effect on the environment only at the end of the transition. The range over which limiting the use of fossil gas binds the optimisation is wider. Compared to other non-renewable fuels, this is due to the longer use of this energy carrier favoured by its low \gls{GWP} (the second after uranium) and its versatility (applications in the electricity, heat and mobility sectors). In line with \citet{vogt2018starting}, the early constraints on generation and the sharper decrease of the emissions avoid lock-in situations.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Binding_learning.pdf}
\caption{Keeping only the successful transitions, distribution of occurrence of binding and not binding actions. Depending on the action and its timing, it is actually constraining the optimisation through EnergyScope Pathway or not. Sweet spots can be identified when considering the limits of \gls{GWP} and fossil gas consumption. Limiting coal consumption is always constraining, unlike \gls{LFO} which is ``naturally'' substituted by EnergyScope Pathway in the near term.}
\label{fig:Binding_learning}
\end{figure}

When it comes to limiting the use of \gls{LFO} and coal, the conclusions are more straightforward. At the beginning of the transition, most of the 159\,TWh of \gls{LFO} are consumed by naphtha-crackers (46\%) and decentralised boilers (45\%). The remaining 10\% are consumed by industrial boilers. Even though \gls{LFO} represents  30\% of the primary energy mix in 2020, the cost-based model removes it from the mix without requiring the action of the agent. Naphtha-crackers, decentralised and industrial boilers get substituted by \gls{MTO}, decentralised \gls{HP} and industrial resistors and \gls{CHP}, respectively.  This ``non-bindness'' of limiting \gls{LFO} is an indication that this action could be removed from the agent's levers of action without impacting the optimisation of its policy.

On the contrary, limiting coal is always binding. Before all, this is due because coal is a cheap resource (17\,€/MWh). In other words, the cost-driven environment will favour it. Then, as the maximum amount of coal (28\,TWh) is much smaller than fossil gas and \gls{LFO}, high values of $\mathrm{act}\textsubscript{coal}$ still represent small consumptions of coal. Whatever the stage in the transition, a policy limiting the use of coal will always be effective. However, to maximise the chance in succeeding the transition, the sooner the better.

\newpage
\section{DISCUSSION AND GUIDELINES FOR FUTURE RESEARCHERS}
\label{sec:discussions}

\acrfull{RL} was found to be an appropriate approach to explore myopic transition pathways under uncertainties and subject to \ce{CO2} budget over this transition. It also allows assessing the robustness of policies to support such pathways. 

As a novice user of a \gls{RL} framework considering continuous action and state spaces, we would recommend opting for \gls{SAC} as it is sample efficient, ensures a wide exploration and has a low sensitivity to hyper-parameters \cite{haarnoja2018soft}. Using the \gls{SAC} package developed by \textsc{Stable-Baselines3} allows a handy introduction to apply \gls{RL}. 

Most of the work in applying \gls{RL} is the definition of the interactions between the agent and its environment (\ie actions, reward and states) that are very dependent on the case study and the research questions to answer. The elements presented in this work result from several trials and errors to end up with meaningful results according to our research questions.

Besides mimicking potential actual policies, the actions chosen in this work have a direct translation into constraints and we can therefore assess their effectiveness through the fact they are binding or not. In this work, we have investigated other actions like incentivising solar \gls{PV} panels and wind turbines by ``artificially'' reducing their \gls{CAPEX}. The result is not conclusive as these technologies must take part in the Belgian energy transition and because it was harder to assess the actual impact of this action. 

The reward function was designed to first aim at respecting the \ce{CO2} budget and then minimising the total transition cost. The -300 penalty given in case of an infeasible optimisation problem was arbitrarily set. \textit{A posteriori}, it seems to be a well-defined penalty given its significant relative difference with the values taken by the reward otherwise, between -120 and 44 (see Figure \ref{fig:reward_status}). Besides this penalty and given the observed results, we recommend starting with the same reward function if the objective is similar, first target cumulative emissions then cumulative costs. This requires defining the \ce{CO2} budget according to a certain sharing principle (see Section \ref{sec:cs:CO2-budget}) and computing the reference total transition cost. However, other research focusing on reaching carbon neutrality by 2050 could define a binary reward function as +1 for reaching the objective and -1 otherwise.

Finally, states aim at representing the information relevant to the agent to efficiently learn and progress through the transitions. For this reason, on top of reward-related features (cumulative emissions and costs), we added other indicators that are actually monitored to help decision-makers assess their policies to reach their targets (share of renewables in the mix and the overall efficiency of the system). For other studies, one might consider other information like the metrics considered by \citet{pickering2022diversity} (\eg heat electrification, average national import or level of curtailment).

In conclusion, future studies might start from the actions-reward-states defined in this work and adapt these rules depending on the research questions to answer, the case study and the energy system optimisation model.

In the context of the energy transition, this thesis aimed at providing decisionmakers with information and new methods considering the intrinsic uncertainties of the future. Even though the following messages result from studies on Belgium, the trends can be transposed to other countries with a high demand and low renewable potentials such as the Netherlands or Germany \cite{thiran2024exploring}.



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%           CONCLUSION %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Experimental procedures}

\subsection{Resource availability}

\subsubsection{Lead Contact}
Further information and requests for resources and materials should be directed to and will be fulfilled by the Lead Contact, Xavier Rixhon (xavier.rixhon@uclouvain.be). 

\subsubsection{Materials availability}
Materials will be deposited to Zenodo {\color{red}(urls will be added)}.

\subsubsection{Data and code availability}
All code and data associated with this study will be available on GitHub and Zenodo {\color{red}(urls will be added)}.

\subsection{Problem formulation and algorithm}
\label{exp_proc:prob_algo}
Before starting an episode, a sample of uncertain parameters is drawn and affects the environment, EnergyScope Pathway, according to the methodology detailed in Section \ref{subsec:uncert_charac}. At the initial state, \ie the energy system in 2020, the agent gets an initial observation, $\bm{o}_0$. An observation represents a set of the characteristics of the environment accessible to the agent for it to take the next action. The state, though, is the exhaustive list of these characteristics. Even though an observation is a subset of the state, this work uses these two words interchangeably. From this state, the agent takes a step: action, reward,  and new state. The action, $\bm{a}_0$, impacts the environment, \ie the energy system limited transition over the first decision window (2020-2030). Through this interaction with its environment, the agent is given a reward, $r_1=r\left(\bm{a}_0 | \bm{o}_0 \right)$, and ends up in a new state, \ie the energy system in 2025, characterised by a new observation, $\bm{o}_1$, and so on (see Figure \ref{fig:Schematics_RL}).

A learning episode is a succession of such learning steps. In the context of the transition pathway between 2020 and 2050, an episode can come to an end for different reasons. First, if the actions taken by the agent make the optimisation infeasible, the episode is prematurely stopped before reaching 2050. Similarly, cumulative emissions of the system over the predefined \ce{CO2} budget (see Section \ref{sec:cs:CO2-budget}) lead to an anticipated end of the episode. Finally, the ``natural'' end is the prescribed end of the transition, \ie 2050. Consequently, the maximum value of steps for an episode is equal to $N=5$. 

Before jumping to the choice of the learning algorithm, it is worth noting that we opted for the combination of \gls{RL} with \gls{DNN}, called \gls{DRL}. Among others, one of the main drawbacks of traditional \gls{RL} algorithms, \ie without the use of \gls{NN},  is that it suffers from the ``curse of dimensionality'' when facing problems with continuous action and state spaces (see Chapter \ref{chap:chap_RL}). By approximating the state-action function with its parameters (\ie weights and biases), \gls{DNN} can address this difficulty. 

Given the assumed absence of knowledge of the agent about the dynamics of the environment, \ie its transition or reward functions, we needed a so-called ``model-free'' learning algorithm. In Reinforcement Learning, the ``model'' stands for the dynamics ``action-state-reward'' between the environment and the agent. In practice, in a model-free approach, the agent estimates the optimal policy directly from experience and without estimating the dynamics of the environment. However, model-free methods suffer from two major drawbacks: their sample inefficiency and their sensitivity concerning their hyper-parameters (\eg learning rates, exploration constants) \cite{haarnoja2018soft}. The former leads to a too-expensive computational burden while the second requires meticulous settings to get good results.  To overcome these two challenges, we needed to choose between an ``on-policy'' or ``off-policy'' algorithm. In a nutshell, in on-policy learning, the agent learns the value function or policy based on the data it generates by following its current policy whereas, in off-policy, the agent can learn from data collected by any policy, not just the one it is currently following, which provides greater flexibility and potential for reusing data stored in the so-called replay buffer. This makes off-policy algorithms more data efficient and ensures better exploration by reusing past experiences or even following random exploration \cite{haarnoja2018soft}.

To optimise the mapping between the observations and the actions, the policy $\pi\left(\bm{a}_n | \bm{o}_n\right)$, an objective function, $\bm{J}(\pi)$, is built on the cumulative rewards collected during each episode. Finally, a back-propagation process updates the weights and biases of the \gls{NN} during the learning of the agent. Among the wide variety of \gls{RL} algorithms applied in energy systems \cite{perera2021applications}, this work opted for \gls{SAC} \cite{haarnoja2018soft} to train and update the \gls{NN}.  Like other actor-critic-based algorithms, \gls{SAC} works with two \gls{NN} in parallel: the actor learning the control policy and the critic judging the actor (see Figure \ref{fig:Actor-critic}).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{Actor-critic.pdf}
\caption{General concept of actor-critic-based algorithms. The two \gls{NN} are trained against each other for the actor to improve the control policy and for the critic to provide a better judgement of the actor's action via the temporal-difference (TD) error. Graph adapted from \cite{cao2020reinforcement}.}
\label{fig:Actor-critic}
\end{figure}

\newpage
\gls{SAC} is a model-free and off-policy actor-critic deep \gls{RL} algorithm based on the entropy-augmented objective function (see Equation (\ref{eq:SAC_objective})). The word ``augmented'' here is in opposition to the conventional \gls{RL} objective function that is only based on the cumulative reward, \ie the first term of Equation (\ref{eq:SAC_objective}). In the \gls{RL} context, entropy, also called ``Shannon entropy'', stands for the randomness or stochasticity of the policy.

\begin{equation}
    \label{eq:SAC_objective}
    \bm{J}(\pi) = \underset{\pi}{\mathbb{E}}\left[\underset{n=0}{\overset{N_{ep}}{\sum}}\gamma^n r_n\left(\bm{o}_n,\bm{a}_n \right) - \zeta \log \left(\pi\left(\bm{a}_n | \bm{o}_n\right) \right) \right],
\end{equation}

\noindent
where $\gamma$ is the discount factor and $\zeta$ the temperature parameter. $\gamma$ determines how much importance we want to give to future rewards within an episode. $\zeta$ balances the trade-off between the exploitation of proven actions via the return maximisation, \ie $\sum_{n=0}^{N_{ep}}\gamma^n r_n\left(\bm{o}_n,\bm{a}_n \right)$, and the exploration through the entropy term, \ie $\log \left(\pi\left(\bm{a}_n | \bm{o}_n\right) \right)$. This way, \gls{SAC} ensures sample efficiency while improving exploration \cite{haarnoja2017reinforcement} and robustness \cite{ziebart2010modeling}. In their work, \citet{haarnoja2017reinforcement} showed a lower sensitivity of \gls{SAC} to hyper-parameters. These make \gls{SAC} a state-of-the-art algorithm and one of the most efficient model-free deep RL methods nowadays \cite{haarnoja2017reinforcement}. In this thesis, we used the open-source \gls{SAC} package developed by \textsc{Stable-Baselines3} \cite{raffin2021stable} where the policy \gls{NN} is a fully connected multilayer perceptron (MLP) built with \textsc{TensorFlow} \cite{abadi2016tensorflow}. For further information on \gls{RL} and the \gls{SAC} algorithm, the interested reader is invited to refer to the works of \citet{sutton2018reinforcement} and \citet{haarnoja2018soft}, respectively.



\section{Supplemental Information}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%           APP METHODO 		 %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection[Reinforcement Learning fundamentals]{Reinforcement Learning fundamentals}
\label{subsec:meth_RL_fundamentals}

\Gls{RL} is a subfield of machine learning focused on training an agent to make sequential decisions by interacting with an environment to achieve specific goals (see Figure \ref{fig:RL_Fundamentals}). Unlike supervised learning, where data is labelled, and unsupervised learning, where patterns are inferred from unlabelled data, reinforcement learning deals with learning from interaction, typically through trial and error. This way, \gls{RL} is considered as active learning \cite{cao2020reinforcement}. Starting from an initial state, the agent takes an action that impacts its environment. The latter feeds back the agent with a reward and the new state. This goes on until the end of the episode. When the episode is done, the agent starts again from an initial state, takes an action and so on. 

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.3\textwidth]{RL_Fundamentals.pdf}
\caption{General concept of \acrfull{RL} as the interactions between the agent and its environment. The agent takes some action that has an impact on the environment which feeds back the agent with a reward and the new state. The objective of the agent is to optimise its policy, \ie the mapping between the state it is at and the action to take, by maximising its cumulative reward.}
\label{fig:RL_Fundamentals}
\end{figure}

The agent learns to optimise its policy by maximising the cumulative reward over time. This policy refers to the strategy or mapping from states to actions that the agent employs to make decisions. Essentially, it defines the behaviour of the agent in the environment. The ultimate goal of the agent is often to find an optimal policy, which maximises the expected cumulative reward over time.  All these concepts and interactions between the agent and its environment are formalised as a \gls{MDP} \cite{sutton2018reinforcement}, represented by the tuple $<s,a,T,r,\pi,\gamma>$.  The Markov property of such a decision process states that a decision is made only based on this tuple and not on the history/path that has led to it. In this tuple, $s\in S$ is the state defined in a certain state space, $S$, that represents the observable parts of the environment that the agent uses to make decisions; $a\in A$ is the action among the action space, $A$; $T$ is the probability of transitioning from one state $s$ to another state $s'$ given a specific action, $a$: $T(s,a,s'): \text{Pr}\left(s'|s,a\right)$; $r$ is the reward received by the agent when taking the action $a$ from state $s$, $r(s,a)$; $\pi$ is the policy telling the action to take depending on the current state and; $\gamma$ is the discount factor that controls the importance of future rewards versus immediate rewards. During the learning/optimisation process, the agent acts according to the exploitation-exploration trade-off. In the exploitation, the action $a$ is directly given by the mapping provided by the current policy $\pi$,  depending on the state $s$. In the exploration, the action is randomly picked within the action space. For further information, the interested reader is invited to refer to the work of \citet{sutton2018reinforcement} or the course given by \citet{davidsilver_RL_online} available online.

\subsection{To bind or not to bind, that is the question}
\label{subsec:binding}
To identify the actions that have an actual impact on the environment, we can check if they are binding or not. In a \gls{LP} problem, constraints represent hyperplanes in the domain of variables. In a two-dimension space, these are straight lines (see Figure \ref{fig:Binding_constr}). When the problem is bounded and feasible, these lines are the edges of a convex polygon: the domain of feasibility. The optimal solution, $\textbf{x}^*$, is the combination of variables leading to the optimal value of the objective function. Besides being within the domain of feasibility, it is proven that this optimal solution, when unique\footnote{There are cases where the objective function has the same optimal value along an entire edge. In this case, there is an infinity of solutions and the problem is indeterminate.}, locates on a vertex of the domain \cite{bertsimas1997introduction}. The constraints intersecting at this vertex are considered binding, actually limiting the objective function to be more optimal. In other words, binding constraints, when tightened, aggravate the objective value function. If these are inequality constraints, as represented in Figure \ref{fig:Binding_constr}, it means that the left and right sides of the equations are equal.

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth]{Binding_constr.pdf}
\caption{Binding versus non-binding constraints. In \gls{LP} where the feasibility domain is non-empty and bounded, the constraints defined a convex feasibility domain in the space of variables (here, x$_1$ and x$_2$). The optimal solution usually locates on a vertex of this domain, \ie the intersection of several constraints (here, constraints 2 and 3) limiting the solution. These constraints are considered binding, \ie having a limiting impact on the optimal solution.}
\label{fig:Binding_constr}
\end{figure} 

\subsection{Comparison with perfect foresight under uncertainties}
\label{subsec:comp_PF}
This section compares these results under myopic conditions supported by the \gls{RL}-agent with the perfect foresight under uncertainties that is considered as a reference.

\gls{RL}-based myopic optimisation provides \ce{CO2} emissions pathways different from the perfect foresight approach to respect the same \ce{CO2} budget (see left side of Figure \ref{fig:Gwp_pathway_total_tran_cost}). However, driven first by this \ce{CO2} budget, the agent often reaches much lower cumulative emissions when succeeding the transition (see Figure \ref{fig:Cum_gwp_cost}). This comes from the agent's actions that limit the emissions and/or the consumption of fossil resources at the early stages. Thanks to the bigger emission reduction at these early stages, the \gls{RL}-based optimisation can benefit from a ``\ce{CO2} buffer'' at the end of the transition. This buffer is compensated by the end of the transition where 50\% of the myopic transitions reach 2050 with 10 or more remaining Mt$_{\ce{CO2},\text{eq}}$ compared to 4 for the perfect foresight approach. These remaining emissions by 2050 come from the consumption in industrial boilers of waste and coal accounting for 3.5\% and 2.4\% of the primary mix on average by 2050. Finally, the long-term vision of the perfect foresight approach results in a smoother reduction of emissions to end up with less emissions by 2050.

The comparison between the failures and the successes demonstrates the added value brought by myopic pathway optimisation. In the near term (2025-2030), levels of emission are similar between perfect foresight and myopic cases that have failed. This shows that limited foresight encourages to strongly act at the early stages. On top of this, following the initial steps of \ce{CO2} emissions pathways resulting from the PF approach would likely ($\sim$80\%) lead to failure of the transition. 

Looking at the total transition cost, the combination of the agent's actions and favourable economic conditions (see Section \ref{subsec:RL:learning:states}) make the myopic transitions cheaper, on average, than the PF cases (see right side of Figure \ref{fig:Gwp_pathway_total_tran_cost}). This is also because the perfect foresight approach always finds a solution even in the worst conditions such as the high cost of purchasing resources and high \gls{EUD}. This explains the wider variability of the PF results too. However, with the same sample of uncertain parameters, given the assumed full knowledge over the whole time horizon, PF naturally results in a cheaper transition than its myopic equivalent.

\begin{figure}[!htbp]
\centering
\includegraphics[height=5cm]{Gwp_pathway_core.pdf}
\includegraphics[height=5cm]{Transition_cost_comp_2.pdf}
\caption{Comparison of \ce{CO2} emissions pathways (left) and total transition cost (right) from the perfect foresight optimisation under uncertainties and the \gls{RL}-based myopic optimisation. Myopic transitions succeed with a more drastic reduction of emissions in the short term and, on average, more favourable economic conditions.}
\label{fig:Gwp_pathway_total_tran_cost}
\end{figure}

The analysis of the cumulative costs shows that the \gls{OPEX} is the main difference between myopic and perfect foresight transitions (see Figure \ref{fig:Opex_Capex_Salvage_comp}). Supported by the agent's actions, successful myopic transitions have a lower \gls{OPEX} than the perfect foresight ones.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.325\textwidth]{Opex_2050_comp_core.pdf}
\includegraphics[width=0.325\textwidth]{Capex_2050_comp_core.pdf}
\includegraphics[width=0.325\textwidth]{Salvage_2050_comp_core.pdf}
\caption{Comparison of cumulative OPEX (left), CAPEX (centre) and salvage value (right) in 2050 from the perfect foresight optimisation under uncertainties and the \gls{RL}-based myopic optimisation.}
\label{fig:Opex_Capex_Salvage_comp}
\end{figure}

The cost of purchasing the energy carriers represents about 70\% of the total cumulative \gls{OPEX}. The assessment of the primary energy mix by 2050 highlights that the difference in OPEX between the perfect foresight and the myopic pathways comes from the import of electrofuels, and especially of e-ammonia (see Figure \ref{fig:Mix_2050_comp}).  In the majority of the cases, e-ammonia is more than two times more imported in the myopic transitions. Being cheaper than e-methane (see Chapter \ref{chap:case_study}), e-ammonia brings flexibility in the production of electricity via \gls{CCGT} (see Chapter \ref{chap:atom_mol}). Besides the slightly favourable economic conditions (see Table \ref{tab:param_RL}), the myopic optimisations opt to invest massively into importing renewable molecules because of the limited knowledge of the future, and, among others, the availability of \gls{SMR}. This explains why 50\% of the successful transitions reached cumulative emissions below 900\,Mt$_{\ce{CO2},\text{eq}}$ (see Figure \ref{fig:Cum_gwp_cost}).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Mix_2050_comp_core.pdf}
\caption{Comparison of the primary energy mix in 2050 from the perfect foresight optimisation under uncertainties and the \gls{RL}-based myopic optimisation. The biggest difference is about e-ammonia to supply \gls{CCGT}.}
\label{fig:Mix_2050_comp}
\end{figure}

\section{Agent-based reinforcement learning to support myopic energy transition}
The principal novelty of this work is the application of \gls{RL} to the optimisation of the transition pathway of a whole-energy system, assuming limited knowledge in the future and a \ce{CO2} budget.  

\section{Definition of the actions, reward and states}
\label{sec:RL:act_states_rew}
The environment with which the \gls{RL}-agent interacts is the optimisation of the transition pathway of a whole-energy system on a specific time window, \eg 2020-2030 then 2025-2035 and so on, until 2040-2050 (see Figure \ref{fig:Schematics_RL}). In a nutshell, starting from the initial state of the environment (\ie the whole-energy system in 2020), the agent takes a set of actions that influence the environment, \ie that affects parameters of the Linear Programming in EnergyScope Pathway. Then, the window 2020-2030 is optimised via EnergyScope. Some of the outputs of this optimisation feed the agent with either the new state of the system or the reward, \ie telling the agent how good the actions were at the state the agent took it. Based on the new state and the reward, the agent takes another set of actions and the window 2025-2035 is optimised. This goes on until 2050.



\subsection{Actions}
\label{subsec:RL:act_states_rew:act}

Defining the levers of action, the core of the policy, to support the transition of a country-size whole-energy system is challenging, especially when accounting for political and socio-technical aspects \cite{castrejon2020making}. In our work, focusing only on the techno-economic aspect, we assume that the actions taken by the agent are directly implemented and impact the environment. In other words, considering only the techno-economic lens, there is no moderation nor contest towards the agent's actions, as the objective is to assess how far and when within the transition to push the different levers of action. Given the overall objective of the agent to succeed the transition, \ie respecting the \ce{CO2} budget by 2050, we have defined the actions in this sense. The first action, $\mathrm{act}_{\mathrm{gwp}} \in [0,1]$, aims at limiting the emissions at the representative year ending the concerned time window, $\textbf{GWP\textsubscript{tot}}(y_{\text{end of the window}})$, between the level of emissions in 2020, \ie $\textbf{GWP\textsubscript{tot}}(2020)=123\,\text{Mt}_{\ce{CO2},\text{eq}}$, and carbon neutrality:

\begingroup
\belowdisplayskip=2pt
\abovedisplayskip=2pt
\begin{flalign} 
\label{eq:RL:act_gwp}
&\textbf{GWP\textsubscript{tot}}(y_{\text{end of the window}})\leq \mathrm{act}_{\mathrm{gwp}} \cdot \textbf{GWP\textsubscript{tot}}(2020). &
\end{flalign}
\endgroup

\noindent
This action is equivalent to setting a national \ce{CO2} quota.

Three additional actions support the strict limitation of yearly emissions: limiting the consumption of oil, fossil gas and coal. Out of the total \gls{GHG} emissions in Belgium in 2020, oil (\ie so-called ``\gls{LFO}'' in the model) and fossil gas account for roughly 40\% and 31\%, respectively. In 2020, solid fossil fuels (\ie so-called ``coal'' in the model) is much less consumed than oil and gas: \ie 28\,TWh of solid fossil fuels versus 159 and 142\,TWh for oil and fossil gas, respectively. Even though its cost (17€/MWh) makes coal cost-competitive, it is a highly-emitting resource, 0.40\,kt$_{\ce{CO2},\text{eq}}$/GWh. For these reasons, three independent actions limit the consumption of these three fossil resources up to the level of consumption in 2020, $\textbf{Cons\textsubscript{fossil gas}}(2020)$, $\textbf{Cons\textsubscript{LFO}}(2020)$ and $\textbf{Cons\textsubscript{coal}}(2020)$,  over the entire concerned time window, except the first one as this year is the initial condition of the time window and cannot be optimised any more:

\begingroup
\belowdisplayskip=2pt
\abovedisplayskip=2pt
\begin{flalign} 
\label{eq:RL:act_NG}
&\textbf{Cons\textsubscript{fossil gas}}(y)\leq \mathrm{act}\textsubscript{fossil gas} \cdot \textbf{Cons\textsubscript{fossil gas}}(2020) & \forall y \in \text{time window}\\
\label{eq:RL:act_LFO}
&\textbf{Cons\textsubscript{LFO}}(y)\leq \mathrm{act}\textsubscript{LFO} \cdot \textbf{Cons\textsubscript{LFO}}(2020) & \forall y \in \text{time window}\\
\label{eq:RL:act_COAL}
&\textbf{Cons\textsubscript{coal}}(y)\leq \mathrm{act}\textsubscript{coal} \cdot \textbf{Cons\textsubscript{coal}}(2020) & \forall y \in \text{time window}
\end{flalign}
\endgroup

\noindent
where $\mathrm{act}\textsubscript{fossil gas}$, $\mathrm{act}\textsubscript{LFO}$ and $\mathrm{act}\textsubscript{coal}$ can take values between 0 and 1. These complete the action space of the agent, $A\in \mathbb{R}^4_{[0,1]}$ (see Figure \ref{fig:Schematic_actions}).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Schematic_actions.pdf}
\caption{Actions available to the decision-maker. Taken at the beginning of the time window to optimise (year $Y$), the four actions impact (i) the emissions of the system at the end of the time window (year $Y+10$) and, (ii-iv) the consumption of fossil gas, LFO and coal at years $Y+5$ and $Y+10$. Unlike the first action that sets a target for the end of the time window, the last three aim at limiting the consumption of these fossil resources over the whole time window.}
\label{fig:Schematic_actions}
\end{figure} 

\subsection{Reward}
\label{subsec:RL:act_states_rew:rew}

When the reward is not properly defined, the agent may optimise its policy for an unintended objective, leading to undesired or suboptimal behaviour, \ie the so-called misalignment of the learning objective \cite{christiano2017deep}. Even worse, it can lead to reward hacking (or reward tampering) where the agent exploits loopholes in the reward function to achieve higher rewards without actually performing the desired task \cite{amodei2016concrete}. On the contrary, a proper definition of the reward function increases the sample efficiency, \ie requiring fewer episodes to converge to the optimal policy.  It also makes the policy more stable and able to withstand variations and uncertainties in the environment \cite{henderson2018deep}.

Through its maximisation of the expected return (see Section \ref{subsec:meth_RL_algo}), a \gls{RL}-agent is as sensitive to positive reward, \ie the carrot, as negative reward, \ie the stick.  When the former encourages desired behaviours, the latter can be seen as a penalty or a punishment and discourages undesirable behaviours \cite{sutton2018reinforcement}. In our case, we have decided to combine these two approaches (see Figure \ref{fig:Reward}).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Reward.pdf}
\caption{Reward function, $R$. Before 2050, the episode is prematurely ended and a negative reward is given if the optimisation is infeasible or if the \ce{CO2} budget is exceeded. If the optimisation provides a solution and the \ce{CO2} budget is not exceeded, the episode continues. Finally, if the episode goes until 2050, the reward is a weighted sum between the capped cumulative emissions and the total transition cost, and the episode terminates. After terminating an episode, the process starts over at the initial state, \ie 2020.}
\label{fig:Reward}
\end{figure} 

\newpage
The reward function is defined in three steps. First of all, taking a set of actions at a certain state might lead to an infeasible optimisation problem. In other words, as actions have a direct impact on some constraints of the problem, they might limit too much the feasible domain to the point where no solution can be found. For instance, the extreme case of aiming at carbon neutrality, \ie $\mathrm{act}_{\mathrm{gwp}}=0$, and forbidding the use of the three aforementioned fossil fuels, \ie $\mathrm{act}\textsubscript{fossil gas}=\mathrm{act}\textsubscript{LFO}=\mathrm{act}\textsubscript{coal}=0$,  from the beginning of the transition makes the optimisation impossible to solve. In this case, the episode is prematurely ended and the reward is ``highly'' negative, -300. If the optimisation is feasible and the end of the transition, \ie 2050, is not reached, the cumulative emissions so far are evaluated. On the one hand, if these cumulative emissions exceed the \ce{CO2} budget, $1.2\,\text{Gt}_{\ce{CO2},\text{eq}}$ (see Section \ref{sec:cs:CO2-budget}), the episode is also ended and a penalisation is given to the agent. This penalisation is proportional to the difference between the \ce{CO2} budget and the actual cumulative emissions.  On the other hand, the episode continues with a zero reward if the \ce{CO2} budget is not exceeded. Eventually, when reaching 2050, given the main objective of the agent to respect the \ce{CO2} budget and not to be more ``\ce{CO2}-ambitious'', we cut short the contribution of the cumulative emissions as soon as they are lower or equal to the \ce{CO2} budget.  On top of that, the reward function includes a secondary objective: the cumulative transition cost. To make the agent sensitive to the cost impact of its policy, we added the total transition cost in the reward function where the \emph{Trans. cost$_{\text{ref}}$} on Figure \ref{fig:Reward} is equal to $1.1\cdot10^3$\,b€. This value comes from the mean of the total transition costs obtained through the \gls{GSA} performed on the perfect foresight transition pathway optimisation (see Section \ref{subsec:atom_mol:results_uq_cost}). In this final form of the reward, one will notice that overshooting cumulative emissions is more penalising than an overshooting transition cost, \ie a weight of 200 for the emissions versus 100 for the cost. The values of these weights are the results of a trial and error to fine-tune the balance between more expensive successes and cheaper failures. This way, we observed that the agent first targeted the respect of the \ce{CO2} budget and then, to a lesser scale, avoided reaching over-costly transitions.

\subsection{States}
\label{subsec:RL:act_states_rew:states}

Besides the reward, the states are the other piece of information provided by the environment to the agent. In \gls{RL}, the purpose of states is to represent the current situation or configuration of the environment in which the agent operates. The primary function of states in RL is to provide the necessary context for the agent to choose appropriate actions based on its current observations and goals \cite{sutton2018reinforcement}. The challenge in the definition of the states is to provide enough information but not too much to avoid overwhelming the agent with non-informative features. 

Consequently, after testing several state spaces and observing the convergence of the reward, we have converged to a four-dimensional state space characterizing the energy system at the end of the optimised time window. The first dimension is directly related to the main objective of the agent: respecting the \ce{CO2} budget until 2050. Therefore, the cumulative emissions emitted so far up to the current step of the transition is the first dimension of the states. Similarly, the cumulative cost of the transition so far constitutes the second dimension of the states to inform the agent about the cost-impact of its actions on the environment. Finally, to enrich the level of details, we have added two other dimensions representative of the key-to-the-transition indicators identified in the Renewable Energy Directive (RED) III of the European Commission \cite{REDIII}: the share of renewables in the primary energy mix and, the energy efficiency. The former is computed as the share of local renewables (\ie wind, solar, hydro and biomass) and imported renewable energy carriers (\ie biofuels and electrofuels) in the total consumption of primary energy. Electricity imported from abroad is not considered in the set of renewable energy carriers even if it can be assumed to be fully renewable by 2050. Finally, even though energy efficiency is usually defined as the ratio between the \gls{FEC} and the primary energy mix, we decided to define this efficiency with a focus on the \gls{EUD}, like in the rest of this thesis. Where electricity, heat and non-energy \gls{EUD} are expressed in terms of energy content, we needed to convert passenger and freight transports into their respective \gls{FEC} to integrate them in the ratio. The information of efficiency fed back by the environment to the agent is the ratio between a ``hybrid'' \gls{EUD} and the consumption of primary energy resources.


\newpage

\section*{Acknowledgement}
Authors acknowledge the support of the Energy Transition Fund of Belgium.

%\section{References}
% If not using biblatex
% To set the bibliography style
%\bibliographystyle{unsrt} 
%\bibliography{bibliography/biblio}{}
% If using biblatex
\printbibliography[heading=bibintoc]
\end{document}
%%%%%%%%% End of the Report %%%%%%%%%