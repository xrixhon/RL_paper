%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%  Welcome to the Cell Press LaTeX template,     
%%%  version 1.10. This is a minimalist template    
%%%  to help you organize your article for            
%%%  publication at Cell Press. PLEASE NOTE:

%%%  (1) If you submit your final manuscript materials 
%%%  in LaTeX format, our typesetters will prepare 
%%%  a Word file for use in production. This conversion 
%%%  process allows us to copyedit your paper, fix 
%%%  any typos, and add formatting and tagging. The 
%%%  conversion process will add approximately 3 
%%%  business days to the production timeline. 
%%%  Authors using LaTeX should keep this in mind 
%%%  when considering deadlines.

%%%  (2) Keep your LaTeX files as simple as possible. 
%%%  Avoid the use of elaborate local macros and/or 
%%%  customized external style files. If you need 
%%%  additional macros, please keep them simple and 
%%%  include them in the actual .tex document preamble. 
%%%  Source code should be set up so that all .sty 
%%%  and .bst files called by the main .tex file are 
%%%  in the same directory as the main .tex file. 

%%%  (3) Cell Press publishes more than 40 journals, 
%%%  some of which may have different or additional 
%%%  formatting requirements not specified in this 
%%%  template. When revising your paper before 
%%%  acceptance, please review the formatting 
%%%  guidelines, including the Final Files Requirements, 
%%%  for the journal you are publishing with.

%%%  Please send feedback on this template to lshipp@cell.com. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\documentclass[12pt,letterpaper]{article}
\usepackage[a4paper, total={7in, 10in}]{geometry}
\renewcommand{\familydefault}{\sfdefault}
\usepackage{graphicx}
\graphicspath{{figures/}}
\usepackage{helvet}
\usepackage{authblk}
\usepackage{hyperref}
\usepackage{amsmath} 
\usepackage{amssymb} 
\usepackage{orcidlink} 
\usepackage[super,comma,sort&compress]  
   {natbib}\bibliographystyle{numbered}
\usepackage[right]{lineno} \linenumbers

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%% OWN PACKAGES %%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{bm} % To use \bm in order to get bold math symbols
\usepackage[version=4]{mhchem}
\usepackage[acronym,nonumberlist]{glossaries} 
\usepackage{glossary-longragged}
\def\eg{e.g., }
\def\ie{i.e., }
\def\og{``}
\def\fg{"}
\usepackage{multirow,booktabs}


%% A
\newacronym{AEO}{AEO}{Annual Energy Outlook}

%% B
\newacronym{BAU}{BAU}{Business-As-Usual}
\newacronym{BEV}{BEV}{Battery Electric Vehicles}

%% C
\newacronym{CAPEX}{CAPEX}{Capital Expenditure}
\newacronym{CCGT}{CCGT}{Combined Cycle Gas Turbine}
\newacronym{CCS}{CCS}{Carbon Capture and Storage}
\newacronym{CEPCI}{CEPCI}{Chemical Engineering's Plant Cost Index}
\newacronym{CHP}{CHP}{Combined Heat and Power}
\newacronym{CNG}{CNG}{Compressed Natural Gas}
\newacronym{CO2}{CO\textsubscript{2}}{Carbon Dioxyde}
\newacronym{COP}{COP}{Coefficient of Performance}

%% D
\newacronym{DEC}{DEC}{Decentralized}
\newacronym{DHN}{DHN}{District Heating Network}
\newacronym{DM}{DM}{Decision-Maker}
\newacronym{DNN}{DNN}{Deep Neural Network}
\newacronym{DRL}{DRL}{Deep Reinforcement Learning}


%% E
\newacronym{EE}{EE}{Elementary Effect}
\newacronym{EIA}{EIA}{Energy Information Administration}
\newacronym{ES}{EnergyScope}{EnergyScope}
\newacronym{ESTD}{EnergyScope TD}{EnergyScope Typical Days}
\newacronym{EO}{EO}{Expert Opinion}
\newacronym{ESOM}{ESOM}{Energy System Optimisation Models}
\newacronym{EU}{EU}{European Union}
\newacronym{EUD}{EUD}{End-Use Demand}
\newacronym{EUT}{EUT}{End-Use Type}
\newacronym{EV}{EV}{Electric Vehicle}

%% F
\newacronym{FC}{FC}{Fuel Cell}
\newacronym{FEC}{FEC}{Final Energy Consumption}

%% G
\newacronym{GHG}{GHG}{Greenhouse Gas}
\newacronym{GSA}{GSA}{Global Sensitivity Analysis}
\newacronym{GtP}{GtP}{Gas-to-Power}
\newacronym{GWP}{GWP}{Global Warming Potential}

%% H
\newacronym{H2}{H2}{Hydrogen}
\newacronym{HEV}{HEV}{Hybrid Electric Vehicle}
\newacronym{HH}{HH}{households}
\newacronym{HP}{HP}{Heat Pump}
\newacronym{HT}{HT}{High-Temperature}
\newacronym{HVC}{HVC}{High Value Chemicals}
\newacronym{HW}{HW}{Hot Water}

%% I
\newacronym{ICE}{ICE}{Internal Combustion Engine}
\newacronym{IEA}{IEA}{International Energy Agency}
\newacronym{IGCC}{IGCC}{Integrated Gasification Combined Cycle}
\newacronym{IPCC}{IPCC}{Intergovernmental Panel for Climate Change}

%% J

%% K
\newacronym{KPI}{KPI}{Key Performance Indicator}

%% L
\newacronym{LCA}{LCA}{Life Cycle Assessment}
\newacronym{LCOE}{LCOE}{Levelised Cost of Energy}
\newacronym{LFO}{LFO}{Light Fuel Oil}
\newacronym{LHV}{LHV}{Lower Heating Value}
\newacronym{LNG}{LNG}{Liquified Natural Gas}
\newacronym{LOO}{LOO}{Leave-One-Out}
\newacronym{LP}{LP}{Linear Programming}
\newacronym{LT}{LT}{Low-Temperature}

%% M
\newacronym{MDP}{MDP}{Markov Decision Process}
\newacronym{MILP}{MILP}{Mixed-Integer Linear Programming}
\newacronym{MOB}{MOB}{Mobility}
\newacronym{MPG}{MPG}{miles-per-gallon}
\newacronym{MSW}{MSW}{Municipal Solid Waste}
\newacronym{MTO}{MTO}{Methanol-to-Olefins}

%% N
\newacronym{NED}{NED}{Non-Energy Demand}
\newacronym{NG}{NG}{Natural Gas}
\newacronym{NN}{NN}{Neural Network}

%% O
\newacronym{OM}{O\&M}{Operation and Maintenance}
\newacronym{Openmod}{Openmod}{Open Energy Modelling Initiative}
\newacronym{OPEX}{OPEX}{Operational Expenditure}
\newacronym{ORC}{ORC}{Organic Rankine cycle}

%% P
\newacronym{PCE}{PCE}{Polynomial Chaos Expansion}
\newacronym{PDF}{PDF}{Probability Density Function}
\newacronym{PF}{PF}{Perfect Foresight}
\newacronym{PHEV}{PHEV}{Plug-in Hybrid Electric Vehicle}
\newacronym{PHS}{PHS}{Pumped Hydro Storage}
\newacronym{pkm}{pkm}{passenger-kilometer}
\newacronym{PtG}{PtG}{Power-to-Gas}
\newacronym{PtH}{PtH}{Power-to-Heat}
\newacronym{PV}{PV}{Photovoltaic}

%% Q

%% R
\newacronym{RL}{RL}{Reinforcement Learning}

%% S
\newacronym{SFOE}{SFOE}{Swiss Federal Office of Energy}
\newacronym{SFOS}{SFOS}{Swiss Federal Office of Statistics}
\newacronym{SH}{SH}{Space Heating}
\newacronym{SMR}{SMR}{Small Modular Reactor}
\newacronym{SNG}{SNG}{Synthetic Natural Gas}
\newacronym{SAC}{SAC}{Soft Actor-Critic}
\newacronym{SPC}{SPC}{Sparse Polynomial Chaos}

%% T
\newacronym{TCO}{TCO}{Total Cost of Ownership}
\newacronym{TD}{TD}{Typical Day}
\newacronym{tkm}{tkm}{ton-kilometer}
\newacronym{TS}{TS}{Thermal Storage}
\newacronym{TSO}{TSO}{Transmission System Operator}

%% U
\newacronym{U-S}{U-S}{Ultra-Supercritical}
\newacronym{UQ}{UQ}{Uncertainty Quantification}

%% V
\newacronym{V2G}{V2G}{Vehicle-to-Grid}
\newacronym{VRES}{VRES}{Variable Renewable Energy Sources}

%% W

%% X

%% Y

%% Z

\usepackage{tablefootnote}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%






\makeatletter
\renewcommand{\maketitle}{\bgroup\setlength{\parindent}{0pt}
\begin{flushleft}
  \textbf{\@title}
  
  \@author
\end{flushleft}\egroup}
\makeatother

%%%  Insert title below; leave date empty.

\title{Supplemental Information:\\
Robustness assessment of energy policies: Reinforcement learning approach to support myopic energy transition}
\date{}

\author[1,2,\orcidlink{0000-0003-2371-8210},*]{Xavier Rixhon}
\author[1]{Hervé Jeanmart}
\author[1]{Francesco Contino}


%%%  Institutional affiliations should contain the 
%%%  following information at minimum: department(s)/
%%%  subunit(s), institution, city, state/region (if 
%%%  applicable), and country. 

\affil[1]{Institute of Mechanics, Materials and Civil Engineering (iMMC), Université catholique de Louvain (UCLouvain), Place du Levant, 2, 1348 Louvain-la-Neuve, Belgium}
\affil[2]{Lead contact}

%%%  List only one email address per corresponding author.

\affil[*]{Correspondence: xavier.rixhon@uclouvain.be}


\begin{document}

\maketitle

\noindent The Supplemental Information begins with

\newpage

\section*{Supplementary Note 1 \hspace{2 mm} Reinforcement Learning fundamentals and algorithm}
\label{sec:meth_RL_fundamentals}

\gls*{RL} is a subfield of machine learning focused on training an agent to make sequential decisions by interacting with an environment to achieve specific goals (see Figure \ref{fig:RL_Fundamentals}). Unlike supervised learning, where data is labelled, and unsupervised learning, where patterns are inferred from unlabelled data, reinforcement learning deals with learning from interaction, typically through trial and error. This way, \gls*{RL} is considered as active learning \cite{cao2020reinforcement}. Starting from an initial state, the agent takes an action that impacts its environment. The latter feeds back the agent with a reward and the new state. This goes on until the end of the episode. When the episode is done, the agent starts again from an initial state, takes an action and so on. 

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.3\textwidth]{RL_Fundamentals.pdf}
\caption{General concept of \acrfull{RL} as the interactions between the agent and its environment. The agent takes some action that has an impact on the environment which feeds back the agent with a reward and the new state. The objective of the agent is to optimise its policy, \ie the mapping between the state it is at and the action to take, by maximising its cumulative reward.}
\label{fig:RL_Fundamentals}
\end{figure}

The agent learns to optimise its policy by maximising the cumulative reward over time. This policy refers to the strategy or mapping from states to actions that the agent employs to make decisions. Essentially, it defines the behaviour of the agent in the environment. The ultimate goal of the agent is often to find an optimal policy, which maximises the expected cumulative reward over time.  All these concepts and interactions between the agent and its environment are formalised as a \gls*{MDP} \cite{sutton2018reinforcement}, represented by the tuple $<s,a,T,r,\pi,\gamma>$.  The Markov property of such a decision process states that a decision is made only based on this tuple and not on the history/path that has led to it. In this tuple, $s\in S$ is the state defined in a certain state space, $S$, that represents the observable parts of the environment that the agent uses to make decisions; $a\in A$ is the action among the action space, $A$; $T$ is the probability of transitioning from one state $s$ to another state $s'$ given a specific action, $a$: $T(s,a,s'): \text{Pr}\left(s'|s,a\right)$; $r$ is the reward received by the agent when taking the action $a$ from state $s$, $r(s,a)$; $\pi$ is the policy telling the action to take depending on the current state and; $\gamma$ is the discount factor that controls the importance of future rewards versus immediate rewards. During the learning/optimisation process, the agent acts according to the exploitation-exploration trade-off. In the exploitation, the action $a$ is directly given by the mapping provided by the current policy $\pi$,  depending on the state $s$. In the exploration, the action is randomly picked within the action space. For further information, the interested reader is invited to refer to the work of \citet{sutton2018reinforcement} or the course given by \citet{davidsilver_RL_online} available online.

Before jumping to the choice of the learning algorithm, it is worth noting that we opted for the combination of \gls*{RL} with \gls*{DNN}, called \gls*{DRL}. Among others, one of the main drawbacks of traditional \gls*{RL} algorithms, \ie without the use of \gls*{NN},  is that it suffers from the ``curse of dimensionality'' when facing problems with continuous action and state spaces (see Chapter \ref{chap:chap_RL}). By approximating the state-action function with its parameters (\ie weights and biases), \gls*{DNN} can address this difficulty. 

Given the assumed absence of knowledge of the agent about the dynamics of the environment, \ie its transition or reward functions, we needed a so-called ``model-free'' learning algorithm. In Reinforcement Learning, the ``model'' stands for the dynamics ``action-state-reward'' between the environment and the agent. In practice, in a model-free approach, the agent estimates the optimal policy directly from experience and without estimating the dynamics of the environment. However, model-free methods suffer from two major drawbacks: their sample inefficiency and their sensitivity concerning their hyper-parameters (\eg learning rates, exploration constants) \cite{haarnoja2018soft}. The former leads to a too-expensive computational burden while the second requires meticulous settings to get good results.  To overcome these two challenges, we needed to choose between an ``on-policy'' or ``off-policy'' algorithm. In a nutshell, in on-policy learning, the agent learns the value function or policy based on the data it generates by following its current policy whereas, in off-policy, the agent can learn from data collected by any policy, not just the one it is currently following, which provides greater flexibility and potential for reusing data stored in the so-called replay buffer. This makes off-policy algorithms more data efficient and ensures better exploration by reusing past experiences or even following random exploration \cite{haarnoja2018soft}.

To optimise the mapping between the observations and the actions, the policy $\pi\left(\bm{a}_n | \bm{o}_n\right)$, an objective function, $\bm{J}(\pi)$, is built on the cumulative rewards collected during each episode. Finally, a back-propagation process updates the weights and biases of the \gls*{NN} during the learning of the agent. Among the wide variety of \gls*{RL} algorithms applied in energy systems \cite{perera2021applications}, this work opted for \gls*{SAC} \cite{haarnoja2018soft} to train and update the \gls*{NN}.  Like other actor-critic-based algorithms, \gls*{SAC} works with two \gls*{NN} in parallel: the actor learning the control policy and the critic judging the actor (see Figure \ref{fig:Actor-critic}).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.5\textwidth]{Actor-critic.pdf}
\caption{General concept of actor-critic-based algorithms. The two \gls*{NN} are trained against each other for the actor to improve the control policy and for the critic to provide a better judgement of the actor's action via the temporal-difference (TD) error. Graph adapted from \cite{cao2020reinforcement}.}
\label{fig:Actor-critic}
\end{figure}

\newpage
\gls*{SAC} is a model-free and off-policy actor-critic deep \gls*{RL} algorithm based on the entropy-augmented objective function (see Equation (\ref{eq:SAC_objective})). The word ``augmented'' here is in opposition to the conventional \gls*{RL} objective function that is only based on the cumulative reward, \ie the first term of Equation (\ref{eq:SAC_objective}). In the \gls*{RL} context, entropy, also called ``Shannon entropy'', stands for the randomness or stochasticity of the policy.

\begin{equation}
    \label{eq:SAC_objective}
    \bm{J}(\pi) = \underset{\pi}{\mathbb{E}}\left[\underset{n=0}{\overset{N_{ep}}{\sum}}\gamma^n r_n\left(\bm{o}_n,\bm{a}_n \right) - \zeta \log \left(\pi\left(\bm{a}_n | \bm{o}_n\right) \right) \right],
\end{equation}

\noindent
where $\gamma$ is the discount factor and $\zeta$ the temperature parameter. $\gamma$ determines how much importance we want to give to future rewards within an episode. $\zeta$ balances the trade-off between the exploitation of proven actions via the return maximisation, \ie $\sum_{n=0}^{N_{ep}}\gamma^n r_n\left(\bm{o}_n,\bm{a}_n \right)$, and the exploration through the entropy term, \ie $\log \left(\pi\left(\bm{a}_n | \bm{o}_n\right) \right)$. This way, \gls*{SAC} ensures sample efficiency while improving exploration \cite{haarnoja2017reinforcement} and robustness \cite{ziebart2010modeling}. In their work, \citet{haarnoja2017reinforcement} showed a lower sensitivity of \gls*{SAC} to hyper-parameters. These make \gls*{SAC} a state-of-the-art algorithm and one of the most efficient model-free deep RL methods nowadays \cite{haarnoja2017reinforcement}. In this thesis, we used the open-source \gls*{SAC} package developed by \textsc{Stable-Baselines3} \cite{raffin2021stable} where the policy \gls*{NN} is a fully connected multilayer perceptron (MLP) built with \textsc{TensorFlow} \cite{abadi2016tensorflow}. For further information on \gls*{RL} and the \gls*{SAC} algorithm, the interested reader is invited to refer to the works of \citet{sutton2018reinforcement} and \citet{haarnoja2018soft}, respectively.


\newpage

\section*{Supplementary Note 2 \hspace{2 mm} Comparison with perfect foresight under uncertainties}
\label{sec:comp_PF}
This section compares these results under myopic conditions supported by the \gls*{RL}-agent with the perfect foresight under uncertainties that is considered as a reference.

\gls*{RL}-based myopic optimisation provides \ce{CO2} emissions pathways different from the perfect foresight approach to respect the same \ce{CO2} budget (see left side of Figure \ref{fig:Gwp_pathway_total_tran_cost}). However, driven first by this \ce{CO2} budget, the agent often reaches much lower cumulative emissions when succeeding the transition (see Figure \ref{fig:Cum_gwp_cost}). This comes from the agent's actions that limit the emissions and/or the consumption of fossil resources at the early stages. Thanks to the bigger emission reduction at these early stages, the \gls*{RL}-based optimisation can benefit from a ``\ce{CO2} buffer'' at the end of the transition. This buffer is compensated by the end of the transition where 50\% of the myopic transitions reach 2050 with 10 or more remaining Mt$_{\ce{CO2},\text{eq}}$ compared to 4 for the perfect foresight approach. These remaining emissions by 2050 come from the consumption in industrial boilers of waste and coal accounting for 3.5\% and 2.4\% of the primary mix on average by 2050. Finally, the long-term vision of the perfect foresight approach results in a smoother reduction of emissions to end up with less emissions by 2050.

The comparison between the failures and the successes demonstrates the added value brought by myopic pathway optimisation. In the near term (2025-2030), levels of emission are similar between perfect foresight and myopic cases that have failed. This shows that limited foresight encourages to strongly act at the early stages. On top of this, following the initial steps of \ce{CO2} emissions pathways resulting from the PF approach would likely ($\sim$80\%) lead to failure of the transition. 

Looking at the total transition cost, the combination of the agent's actions and favourable economic conditions (see Section \ref{subsec:RL:learning:states}) make the myopic transitions cheaper, on average, than the PF cases (see right side of Figure \ref{fig:Gwp_pathway_total_tran_cost}). This is also because the perfect foresight approach always finds a solution even in the worst conditions such as the high cost of purchasing resources and high \gls*{EUD}. This explains the wider variability of the PF results too. However, with the same sample of uncertain parameters, given the assumed full knowledge over the whole time horizon, PF naturally results in a cheaper transition than its myopic equivalent.

\begin{figure}[!htbp]
\centering
\includegraphics[height=5cm]{Gwp_pathway_core.pdf}
\includegraphics[height=5cm]{Transition_cost_comp_2.pdf}
\caption{Comparison of \ce{CO2} emissions pathways (left) and total transition cost (right) from the perfect foresight optimisation under uncertainties and the \gls*{RL}-based myopic optimisation. Myopic transitions succeed with a more drastic reduction of emissions in the short term and, on average, more favourable economic conditions.}
\label{fig:Gwp_pathway_total_tran_cost}
\end{figure}

The analysis of the cumulative costs shows that the \gls*{OPEX} is the main difference between myopic and perfect foresight transitions (see Figure \ref{fig:Opex_Capex_Salvage_comp}). Supported by the agent's actions, successful myopic transitions have a lower \gls*{OPEX} than the perfect foresight ones.

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.325\textwidth]{Opex_2050_comp_core.pdf}
\includegraphics[width=0.325\textwidth]{Capex_2050_comp_core.pdf}
\includegraphics[width=0.325\textwidth]{Salvage_2050_comp_core.pdf}
\caption{Comparison of cumulative OPEX (left), CAPEX (centre) and salvage value (right) in 2050 from the perfect foresight optimisation under uncertainties and the \gls*{RL}-based myopic optimisation.}
\label{fig:Opex_Capex_Salvage_comp}
\end{figure}

The cost of purchasing the energy carriers represents about 70\% of the total cumulative \gls*{OPEX}. The assessment of the primary energy mix by 2050 highlights that the difference in OPEX between the perfect foresight and the myopic pathways comes from the import of electrofuels, and especially of e-ammonia (see Figure \ref{fig:Mix_2050_comp}).  In the majority of the cases, e-ammonia is more than two times more imported in the myopic transitions. Being cheaper than e-methane (see Chapter \ref{chap:case_study}), e-ammonia brings flexibility in the production of electricity via \gls*{CCGT} (see Chapter \ref{chap:atom_mol}). Besides the slightly favourable economic conditions (see Table \ref{tab:param_RL}), the myopic optimisations opt to invest massively into importing renewable molecules because of the limited knowledge of the future, and, among others, the availability of \gls*{SMR}. This explains why 50\% of the successful transitions reached cumulative emissions below 900\,Mt$_{\ce{CO2},\text{eq}}$ (see Figure \ref{fig:Cum_gwp_cost}).

\begin{figure}[!htbp]
\centering
\includegraphics[width=0.8\textwidth]{Mix_2050_comp_core.pdf}
\caption{Comparison of the primary energy mix in 2050 from the perfect foresight optimisation under uncertainties and the \gls*{RL}-based myopic optimisation. The biggest difference is about e-ammonia to supply \gls*{CCGT}.}
\label{fig:Mix_2050_comp}
\end{figure}

\newpage

\section*{Supplementary Note 3 \hspace{2 mm} To bind or not to bind, that is the question}
\label{sec:binding}
To identify the actions that have an actual impact on the environment, we can check if they are binding or not. In a \gls*{LP} problem, constraints represent hyperplanes in the domain of variables. In a two-dimension space, these are straight lines (see Figure \ref{fig:Binding_constr}). When the problem is bounded and feasible, these lines are the edges of a convex polygon: the domain of feasibility. The optimal solution, $\textbf{x}^*$, is the combination of variables leading to the optimal value of the objective function. Besides being within the domain of feasibility, it is proven that this optimal solution, when unique\footnote{There are cases where the objective function has the same optimal value along an entire edge. In this case, there is an infinity of solutions and the problem is indeterminate.}, locates on a vertex of the domain \cite{bertsimas1997introduction}. The constraints intersecting at this vertex are considered binding, actually limiting the objective function to be more optimal. In other words, binding constraints, when tightened, aggravate the objective value function. If these are inequality constraints, as represented in Figure \ref{fig:Binding_constr}, it means that the left and right sides of the equations are equal.

\begin{figure}[!t]
\centering
\includegraphics[width=0.7\textwidth]{Binding_constr.pdf}
\caption{Binding versus non-binding constraints. In \gls*{LP} where the feasibility domain is non-empty and bounded, the constraints defined a convex feasibility domain in the space of variables (here, x$_1$ and x$_2$). The optimal solution usually locates on a vertex of this domain, \ie the intersection of several constraints (here, constraints 2 and 3) limiting the solution. These constraints are considered binding, \ie having a limiting impact on the optimal solution.}
\label{fig:Binding_constr}
\end{figure}

\bibliography{references}

\bigskip

%%%  In your References, please include only articles 
%%%  that are published (online publication and 
%%%  preprint servers are OK). Unpublished data, 
%%%  submitted and/or accepted manuscripts, abstracts, 
%%%  and personal communications should be cited within 
%%%  the text only ("unpublished data," "data not 
%%%  shown," "Alice Smith, personal communication") 
%%%  and not included in the references list. Personal 
%%%  communication should be documented by a letter 
%%%  of permission. Whenever possible, please make 
%%%  sure your .bib file has the complete author lists 
%%%  for each item (at minimum, the first 11 authors 
%%%  listed). 

\end{document}